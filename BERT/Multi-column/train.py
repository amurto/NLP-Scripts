# -*- coding: utf-8 -*-
"""BERT_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UbYB9XAAcumx3wz2FFirnvt3hM_6dV5Y
"""

# install mxnet-cu100 because google colab supports version 100, if gpu not available run "!pip install mxnet" instead
!pip install mxnet-cu100
!pip install gluonnlp

# unzip sentence_embedding from gluonnlp which contains our bert model
!unzip sentence_embedding.zip

# These training examples give error, because they contain inconsistencies, so remove them from dataset
to_ignore = to_ignore = [232,
 275,
 957,
 1129,
 1322,
 1722,
 1865,
 1891,
 1900,
 2398,
 2447,
 3180,
 3263,
 3270,
 3465,
 3520,
 3656,
 4079,
 4419,
 4439,
 4498,
 4754,
 5074,
 5172,
 5236,
 5672,
 6140,
 6258,
 6777,
 6784,
 6830,
 7046,
 7380,
 7493,
 7516,
 7596,
 7621,
 7883,
 8135,
 8762,
 8867,
 9296,
 9386,
 9581,
 9757,
 9766,
 10121,
 10319,
 10463,
 10895,
 10917,
 10961,
 11296,
 11524,
 11745,
 12149,
 12560,
 12705,
 12788,
 13160,
 13311,
 13784,
 13943,
 14059,
 14540,
 14651,
 14720,
 15015,
 15551,
 15618,
 15837,
 15866,
 16034,
 16179,
 16260,
 16403,
 16500,
 16532,
 16668,
 16884,
 17050,
 17257,
 17423,
 17795,
 17802,
 17817,
 18311,
 18409,
 18461,
 18978,
 19087,
 19494,
 19602,
 20258,
 20557,
 21672,
 21713,
 21832,
 22045,
 22184,
 22259,
 22299,
 22426,
 22568,
 23028,
 23198,
 23944,
 24212,
 24648,
 25447,
 25663,
 25740,
 25790,
 25874,
 25951,
 26231,
 26839,
 27006,
 27364,
 27935,
 28245,
 28248,
 28676,
 29120,
 29260,
 29408,
 30026,
 30105,
 30227,
 30433,
 30695,
 30702,
 30864,
 30933,
 31223,
 31299,
 31831,
 31854,
 32013,
 32610,
 33187,
 33282,
 33738,
 33878,
 34406,
 34464,
 34496,
 35239,
 35578,
 35726,
 35783,
 35917,
 36021,
 36123,
 36400,
 36577,
 36809,
 37061,
 37125,
 37186,
 37563,
 37578,
 37608,
 37823,
 38022,
 38197,
 38492,
 38684,
 38725,
 38798,
 39288,
 39314,
 39681,
 39925,
 40029,
 40135,
 40383,
 40499,
 40828,
 40971,
 41389,
 41510,
 41768,
 41991,
 42166,
 42183,
 42779,
 43736,
 44485,
 44593,
 44711,
 45434,
 45562,
 45645,
 45980,
 46099,
 46308,
 46731,
 46801,
 46829,
 46892,
 47491,
 47582,
 48036,
 48177,
 48310,
 48355,
 48637,
 49375]

import csv
import pandas as pd

train_bodies = pd.read_csv('train_bodies.csv')
train_topics = pd.read_csv('train_stances.csv')
train_labels = train_topics['Stance'].values
train_dataset = pd.merge(train_topics, train_bodies, how='left', on='Body ID')
train_dataset.drop(train_dataset.index[to_ignore], inplace=True)
train_dataset = train_dataset[['Headline', 'articleBody', 'Stance']]
ibm_dataset = pd.read_csv('ibm_dataset.csv')
train_dataset = pd.concat([train_dataset, ibm_dataset])

train_dataset.to_csv('train_bert.csv', index=False, header=False)
csv.writer(open('train_bert.tsv', 'w+'), delimiter='\t').writerows(csv.reader(open("train_bert.csv")))

# tsv format is required for gluonnlp 
!mv train_bert.tsv sentence_embedding/

cd sentence_embedding/

import warnings
warnings.filterwarnings('ignore')

import io
import random
import numpy as np
import mxnet as mx
import gluonnlp as nlp
from bert import data, model

np.random.seed(100)
random.seed(100)
mx.random.seed(10000)
# change `ctx` to `mx.cpu()` if no GPU is available.
ctx = mx.gpu(0)

# Automatically downloads and loads bert uncased model
bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',
                                             dataset_name='book_corpus_wiki_en_uncased',
                                             pretrained=True, ctx=ctx, use_pooler=True,
                                             use_decoder=False, use_classifier=False)
print(bert_base)

# Attach a single classifier layer on top of language model

bert_classifier = model.classification.BERTClassifier(bert_base, num_classes=4, dropout=0.1)
# only need to initialize the classifier layer.
bert_classifier.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)
bert_classifier.hybridize(static_alloc=True)

# softmax cross entropy loss for classification
loss_function = mx.gluon.loss.SoftmaxCELoss()
loss_function.hybridize(static_alloc=True)

metric = mx.metric.Accuracy()

tsv_file = io.open('train_bert.tsv', encoding='utf-8', newline='\r\n')
for i in range(5):
    print(tsv_file.readline())

# Modify newline parameter to support news articles loading which contains windows type newlines
class modifyread(nlp.data.TSVDataset):
    def _read(self):
        all_samples = []
        for filename in self._filenames:
            with io.open(filename, 'r', encoding=self._encoding, newline='\r\n') as fin:
                content = fin.read()
            samples = (s for s in self._sample_splitter(content) if not self._should_discard())
            if self._field_separator:
                if not self._allow_missing:
                    samples = [self._field_selector(self._field_separator(s)) for s in samples]
                else:
                    selected_samples = []
                    num_missing = 0
                    for s in samples:
                        try:
                            fields = self._field_separator(s)
                            selected_samples.append(self._field_selector(fields))
                        except IndexError:
                            num_missing += 1
                    if num_missing > 0:
                        warnings.warn('%d incomplete samples in %s'%(num_missing, filename))
                    samples = selected_samples
            all_samples += samples
        return all_samples

# TO skip the first line, in case of headers, change the value to 1 below
num_discard_samples = 0

# Split fields by tabs
field_separator = nlp.data.Splitter('\t')

data_train_raw = modifyread(filename='train_bert.tsv', sample_splitter=nlp.data.utils.Splitter('\r\n'),
                                 field_separator=field_separator,
                                 num_discard_samples=num_discard_samples,
                                 field_indices=None)
sample_id = 231
# Headline
print(data_train_raw[sample_id][0])
# Articles
print(data_train_raw[sample_id][1])
# Stance
print(data_train_raw[sample_id][2])

################ Used to find inconsistent examples, no longer needed, just for reference #####################################
'''indices = []
for item in data_train_raw:
    try:
        indices.append(item[2])
    except IndexError:
        pass

to_ignore = []
for i,item in enumerate(indices):
    if len(item) > 9:
        to_ignore.append(i)
'''

# Use the vocabulary from pre-trained model for tokenization
bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)

# The maximum length of an input sequence
max_len = 200

# The labels for the four classes
all_labels = ["agree", "disagree", "discuss", "unrelated"]

# Transform the data as sentence pairs.
pair = True
transform = data.transform.BERTDatasetTransform(bert_tokenizer, max_len,
                                                class_labels=all_labels,
                                                has_label=True,
                                                pad=True,
                                                pair=pair)
data_train = data_train_raw.transform(transform)

print('vocabulary used for tokenization = \n%s'%vocabulary)
print('%s token id = %s'%(vocabulary.padding_token, vocabulary[vocabulary.padding_token]))
print('%s token id = %s'%(vocabulary.cls_token, vocabulary[vocabulary.cls_token]))
print('%s token id = %s'%(vocabulary.sep_token, vocabulary[vocabulary.sep_token]))
print('token ids = \n%s'%data_train[sample_id][0])
print('valid length = \n%s'%data_train[sample_id][1])
print('segment ids = \n%s'%data_train[sample_id][2])
print('label = \n%s'%data_train[sample_id][3])

# The hyperparameters
batch_size = 20
lr = 5e-6

# The FixedBucketSampler and the DataLoader for making the mini-batches
train_sampler = nlp.data.FixedBucketSampler(lengths=[int(item[1]) for item in data_train],
                                            batch_size=batch_size,
                                            shuffle=True)
bert_dataloader = mx.gluon.data.DataLoader(data_train, batch_sampler=train_sampler)

trainer = mx.gluon.Trainer(bert_classifier.collect_params(), 'adam',
                           {'learning_rate': lr, 'epsilon': 1e-9})

# Collect all differentiable parameters
# `grad_req == 'null'` indicates no gradients are calculated (e.g. constant parameters)
# The gradients for these params are clipped later
params = [p for p in bert_classifier.collect_params().values() if p.grad_req != 'null']
grad_clip = 1

# Training the model with only two epochs to avoid overfitting
log_interval = 4
num_epochs = 2
for epoch_id in range(num_epochs):
    metric.reset()
    step_loss = 0
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(bert_dataloader):
        with mx.autograd.record():

            # Load the data to the GPU
            token_ids = token_ids.as_in_context(ctx)
            valid_length = valid_length.as_in_context(ctx)
            segment_ids = segment_ids.as_in_context(ctx)
            label = label.as_in_context(ctx)

            # Forward computation
            out = bert_classifier(token_ids, segment_ids, valid_length.astype('float32'))
            ls = loss_function(out, label).mean()

        # And backwards computation
        ls.backward()

        # Gradient clipping
        trainer.allreduce_grads()
        nlp.utils.clip_grad_global_norm(params, 1)
        trainer.update(1)

        step_loss += ls.asscalar()
        metric.update([label], [out])

        # Printing vital information
        if (batch_id + 1) % (log_interval) == 0:
            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}'
                         .format(epoch_id, batch_id + 1, len(bert_dataloader),
                                 step_loss / log_interval,
                                 trainer.learning_rate, metric.get()[1]))
            step_loss = 0

cd ..

# similar to training examples, some test examples are not properly loaded so remove them
to_remove = [776,
 1898,
 5433,
 10487,
 10924,
 11796,
 13534,
 14205,
 17744,
 18082,
 19655,
 20866,
 21043,
 21395]

##################################################### Run on test data and generate predictions ################################################################

test_bodies = pd.read_csv('test_bodies.csv')
test_topics = pd.read_csv('test_stances.csv')
test_dataset = pd.merge(test_topics, test_bodies, how='left', on='Body ID')
test_dataset.drop(test_dataset.index[to_remove], inplace=True)
test_labels = test_dataset['Stance'].values

# testing for only 1000 examples currently
test_dataset = test_dataset[['Headline', 'articleBody']][:5000]
test_labels = test_labels[:5000]

test_dataset.to_csv('test_bert.csv', index=False, header=False)
csv.writer(open('test_bert.tsv', 'w+'), delimiter='\t').writerows(csv.reader(open("test_bert.csv")))

!mv test_bert.tsv sentence_embedding/

cd sentence_embedding

# TO skip the first line, in case of headers, change the value to 1 below
num_discard_samples = 0

# Split fields by tabs
field_separator = nlp.data.Splitter('\t')

data_test_raw = modifyread(filename='test_bert.tsv', sample_splitter=nlp.data.utils.Splitter('\r\n'),
                                 field_separator=field_separator,
                                 num_discard_samples=num_discard_samples,
                                 field_indices=None)
sample_id = 231
# Headline
print(data_test_raw[sample_id][0])
# Articles
print(data_test_raw[sample_id][1])

###################  Used to find inconsistent examples, no longer needed, just for reference ###############################
'''
to_remove = []
for i,item in enumerate(data_test_raw):
    if len(item) != 2:
        to_remove.append(i)
'''

test_transform = data.transform.BERTDatasetTransform(bert_tokenizer, max_len,
                                                class_labels=False,
                                                has_label=False,
                                                pad=True,
                                                pair=pair)
data_test = data_test_raw.transform(test_transform)

print('vocabulary used for tokenization = \n%s'%vocabulary)
print('%s token id = %s'%(vocabulary.padding_token, vocabulary[vocabulary.padding_token]))
print('%s token id = %s'%(vocabulary.cls_token, vocabulary[vocabulary.cls_token]))
print('%s token id = %s'%(vocabulary.sep_token, vocabulary[vocabulary.sep_token]))
print('token ids = \n%s'%data_test[sample_id][0])
print('valid length = \n%s'%data_test[sample_id][1])
print('segment ids = \n%s'%data_test[sample_id][2])

test_dataloader = mx.gluon.data.DataLoader(data_test,batch_size=batch_size, num_workers=4)

# This gives weird assertion error at the last but the result is not affected, so we wrap this in try and except
try:
    predictions = []
    for batch_id, (token_ids, valid_length, segment_ids) in enumerate(test_dataloader):
        # Load the data to the GPU
        token_ids = token_ids.as_in_context(ctx)
        valid_length = valid_length.as_in_context(ctx)
        segment_ids = segment_ids.as_in_context(ctx)

        # Forward computation
        out = bert_classifier(token_ids, segment_ids, valid_length.astype('float32'))

        indices = mx.nd.topk(out, k=1, ret_typ='indices', dtype='int32').asnumpy()
        for index in indices:
            predictions.append(index)
except:
    pass

from sklearn import metrics
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
encoder.fit(test_labels)
encoded_test_labels = encoder.transform(test_labels)

acc = metrics.accuracy_score(encoded_test_labels, predictions)

print(f"Accuracy obtained on test set: {acc}")

print(metrics.confusion_matrix(encoded_test_labels, predictions))

bert_classifier.save_parameters('parameters_bert')