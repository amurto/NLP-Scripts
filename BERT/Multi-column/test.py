# -*- coding: utf-8 -*-
"""stancetest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H-ouQbECShOjjLL7K3MS4csCQZmYQ4i5
"""

from google.colab import drive
drive.mount('/content/drive')

# install mxnet-cu100 because google colab supports version 100, if gpu not available run "!pip install mxnet" instead
!pip install mxnet-cu100
!pip install gluonnlp

# unzip sentence_embedding from gluonnlp which contains our bert model
!unzip sentence_embedding.zip

!mv parameters_bert sentence_embedding/

cd /content/drive/My Drive/work/News Stance Detection/Production/sentence_embedding

import warnings
warnings.filterwarnings('ignore')

import io
import random
import numpy as np
import mxnet as mx
import gluonnlp as nlp
from bert import data, model
import csv
import json

np.random.seed(100)
random.seed(100)
mx.random.seed(10000)
# change `ctx` to `mx.cpu()` if no GPU is available.
ctx = mx.gpu(0)

# Automatically downloads and loads bert uncased model
bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',
                                             dataset_name='book_corpus_wiki_en_uncased',
                                             pretrained=True, ctx=ctx, use_pooler=True,
                                             use_decoder=False, use_classifier=False)
print(bert_base)

# Attach a single classifier layer on top of language model

bert_classifier = model.classification.BERTClassifier(bert_base, num_classes=4, dropout=0.1)
# only need to initialize the classifier layer.
bert_classifier.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)
bert_classifier.hybridize(static_alloc=True)

# softmax cross entropy loss for classification
loss_function = mx.gluon.loss.SoftmaxCELoss()
loss_function.hybridize(static_alloc=True)

metric = mx.metric.Accuracy()

# Modify newline parameter to support news articles loading which contains windows type newlines
class modifyread(nlp.data.TSVDataset):
    def _read(self):
        all_samples = []
        for filename in self._filenames:
            with io.open(filename, 'r', encoding=self._encoding, newline='\r\n') as fin:
                content = fin.read()
            samples = (s for s in self._sample_splitter(content) if not self._should_discard())
            if self._field_separator:
                if not self._allow_missing:
                    samples = [self._field_selector(self._field_separator(s)) for s in samples]
                else:
                    selected_samples = []
                    num_missing = 0
                    for s in samples:
                        try:
                            fields = self._field_separator(s)
                            selected_samples.append(self._field_selector(fields))
                        except IndexError:
                            num_missing += 1
                    if num_missing > 0:
                        warnings.warn('%d incomplete samples in %s'%(num_missing, filename))
                    samples = selected_samples
            all_samples += samples
        return all_samples

def test():
  h = input("Enter headline ")
  b = input("Enter body ")
  import pandas as pd 
    
  # intialise data of lists. 
  text_input = {'Headline':[h], 'articleBody':[b]} 

  class_labels = ["Agree", "Disagree", "Discuss", "Unrelated"]

  # Create DataFrame 
  df = pd.DataFrame(text_input)

  df.to_csv('input.csv', index=False, header=False)
  csv.writer(open('input.tsv', 'w+'), delimiter='\t').writerows(csv.reader(open("input.csv")))

  # TO skip the first line, in case of headers, change the value to 1 below
  num_discard_samples = 0

  # Split fields by tabs
  field_separator = nlp.data.Splitter('\t')

  input_raw = modifyread(filename='input.tsv', sample_splitter=nlp.data.utils.Splitter('\r\n'),
                                  field_separator=field_separator,
                                  num_discard_samples=num_discard_samples,
                                  field_indices=None)

  # Use the vocabulary from pre-trained model for tokenization
  bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)

  # The maximum length of an input sequence
  max_len = 200

  # Transform the data as sentence pairs.
  pair=True
  test_transform = data.transform.BERTDatasetTransform(bert_tokenizer, max_len,
                                                  class_labels=False,
                                                  has_label=False,
                                                  pad=True,
                                                  pair=pair)
  data_test = input_raw.transform(test_transform)
  test_dataloader = mx.gluon.data.DataLoader(data_test,batch_size=1, num_workers=4)
  bert_classifier.load_parameters('parameters_bert', ctx=ctx)

  # This gives weird assertion error at the last but the result is not affected, so we wrap this in try and except
  try:
      for batch_id, (token_ids, valid_length, segment_ids) in enumerate(test_dataloader):
          # Load the data to the GPU
          token_ids = token_ids.as_in_context(ctx)
          valid_length = valid_length.as_in_context(ctx)
          segment_ids = segment_ids.as_in_context(ctx)

          # Forward computation
          out = bert_classifier(token_ids, segment_ids, valid_length.astype('float32'))
          
          indices = mx.nd.topk(out, k=1, ret_typ='indices', dtype='int32').asnumpy()
          predictions = indices[0][0]
          confidence_scores = out[0].asnumpy()
          thisdict = {
              "Stance": class_labels[predictions],
              "Confidence_Scores" : {
                  class_labels[0] : (float(confidence_scores[0]) + 10)/20,
                  class_labels[1] : (float(confidence_scores[1]) + 10)/20,
                  class_labels[2] : (float(confidence_scores[2]) + 10)/20,
                  class_labels[3] : (float(confidence_scores[3]) + 10)/20               
            }
          }
          
  except:
      pass

  print(thisdict)
  with open('result.json', 'a') as fp:
        json.dump(thisdict, fp, indent=4)

test()