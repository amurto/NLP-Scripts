# -*- coding: utf-8 -*-
"""agg_bert_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T2jU-Rc5147atbRLZfZSJcy8NnDvRQyO
"""

from google.colab import drive
drive.mount('/content/drive')

! pip install transformers

import os
from typing import Tuple, List
from functools import partial
import json
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, RandomSampler
from torch.nn.utils.rnn import pad_sequence
from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, BertPreTrainedModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from tqdm import tqdm

bert_model_name = 'bert-base-cased'
path = "./"
device = torch.device('cpu')
if torch.cuda.is_available():
    device = torch.device('cuda:0')
tokenizer = BertTokenizer.from_pretrained(bert_model_name)
assert tokenizer.pad_token_id == 0, "Padding value used in masks is set to zero, please change it everywhere"

class BertClassifier(nn.Module):
    
    def __init__(self, bert: BertModel, num_classes: int):
        super().__init__()
        self.bert = bert
        self.classifier = nn.Linear(bert.config.hidden_size, num_classes)
        
    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,
                
            labels=None):
        outputs = self.bert(input_ids,
                               attention_mask=attention_mask,
                               token_type_ids=token_type_ids,
                               position_ids=position_ids,
                               head_mask=head_mask)
        cls_output = outputs[1] # batch, hidden
        cls_output = self.classifier(cls_output) # batch, 6
        cls_output = torch.sigmoid(cls_output)
        criterion = nn.BCELoss()
        loss = 0
        if labels is not None:
            loss = criterion(cls_output, labels)
        return loss, cls_output

model = torch.load("/content/drive/My Drive/New KS/tensor.pt", map_location=torch.device('cpu'))
model.eval()

def test():
  sen = input("Enter text ")
  sent = []
  sent.append(sen)
  texts = []
  for text in sent:
      text = tokenizer.encode(text, add_special_tokens=True)
      if len(text) > 120:
          text = text[:119] + [tokenizer.sep_token_id]
      texts.append(torch.LongTensor(text))
  x = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)
  mask = (x != tokenizer.pad_token_id).float().to(device)
  with torch.no_grad():
    _, outputs = model(x, attention_mask=mask)
  outputs = outputs.cpu().numpy()
  agg_dict = {"CAG" : float(outputs[0][0]), 
              "NAG" : float(outputs[0][1]),
              "OAG" : float(outputs[0][2])}
  print(json.dumps(agg_dict))

test()