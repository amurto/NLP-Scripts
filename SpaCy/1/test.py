# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VtBmRm8AlpyxIbe9xpCT-h9h1ub298H0
"""

# Download the StanfordCoreNLP package
# We will use this package to extract verb phrases from sentences
!wget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip

# Unzip the stanfordCoreNLP package
import zipfile

fh = open('stanford-corenlp-full-2018-10-05.zip', 'rb')
z = zipfile.ZipFile(fh)
for name in z.namelist():
    outpath = "/content/drive/My Drive/cicd/Production/stanfordcorenlp"
    z.extract(name, outpath)
fh.close()

pip install textacy

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import spacy
from spacy import displacy
from spacy.util import minibatch, compounding
from __future__ import unicode_literals
import spacy,en_core_web_sm
import textacy
import json

import matplotlib.pyplot as plt
# %matplotlib inline

from stanfordcorenlp import StanfordCoreNLP
from nltk.tree import Tree
import re

checkvp = StanfordCoreNLP('/content/drive/My Drive/cicd/Production/stanfordcorenlp/stanford-corenlp-full-2018-10-05')

output_dir = "/content/drive/My Drive/cicd/Production/spacy"

# test the saved model
print("Loading from", output_dir)
nlp = spacy.load(output_dir)

# form a parse tree of the sentence
# Extract the verb phrases
def extract_phrase(tree_str, label):
    phrases = []
    trees = Tree.fromstring(tree_str)
    for tree in trees:
        for subtree in tree.subtrees():
            if subtree.label() == label:
                t = subtree
                t = ' '.join(t.leaves())
                phrases.append(t)

    return phrases

# Check if the sentence contains a verb phrase
def is_vp(sentence):
  tree_str = checkvp.parse(sentence)
  vps = extract_phrase(tree_str, 'VP')
  if len(vps) == 0:
    return False
  else:
    vpl = vps[0].split(" ")
    if len(vpl) > 1:
      return True
    else: 
      return False

# Tokenize the text into sentences
# Classify whether a sentence is a claim or not
# Iterate through the non-claims after removing symbols and check for claims
# Return a list of sentences classified as claims from the sentences. 

def get_claims(text):
  from nltk import tokenize
  sentence_raw = tokenize.sent_tokenize(text)
  sentences = []
  for sentence in sentence_raw:
    if is_vp(sentence):
      sentences.append(sentence)
  claims = []
  for sent in sentences:
    doc = nlp(sent)
    if doc.cats.get("CLAIM_PER") >= 0.7:
      claims.append(sent)
    else:
      simple_sent = re.split('; |, |\*|\n|- ',sent)
      for sent in simple_sent:
        if is_vp(sent):
          doc = nlp(sent)
          if doc.cats.get("CLAIM_PER") >= 0.7:
            claims.append(sent)
  return claims

# Create a dictionary from the raw text and claims
# JSONify the dictionary
def get_json(text, claims):
  thisdict = {
    "text": text,
    "claims": claims
  }
  text_and_claims = json.dumps(thisdict)
  return text_and_claims

# Take text as input from the user
# Print the output
def test():
  text = input("Enter text: ")
  claims = get_claims(text)
  result = get_json(text, claims)
  print(result)

test()