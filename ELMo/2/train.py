# -*- coding: utf-8 -*-
"""elmotrainnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O5EbhfvKAXjyy3YNx8w235tUeZqhH-zL
"""

from google.colab import drive
drive.mount('/content/drive')

# Import dependencies
import tensorflow as tf
import pandas as pd
import tensorflow_hub as hub
import numpy as np
import re
import keras
import keras.backend as K
from keras.layers import Dense, Embedding, Input
from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, Lambda
from keras.models import Model
from keras.preprocessing import text, sequence
from imblearn.under_sampling import RandomUnderSampler
from sklearn import metrics
import spacy

dataset = pd.read_csv('crtrain.csv')

from sklearn.utils import shuffle
dataset = shuffle(dataset)



dataset

train = dataset

# remove URL's from train and test
train['clean_sent'] = train['sentences'].apply(lambda x: re.sub(r'http\S+', '', x))

# remove punctuation marks
punctuation = '!"#$%&()*+-/:;<=>?@[\\]^_`{|}~'

train['clean_sent'] = train['clean_sent'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))

# convert text to lowercase
train['clean_sent'] = train['clean_sent'].str.lower()

# remove whitespaces
train['clean_sent'] = train['clean_sent'].apply(lambda x:' '.join(x.split()))

# import spaCy's language model
nlp = spacy.load('en', disable=['parser', 'ner'])

# function to lemmatize text
def lemmatization(texts):
    output = []
    for i in texts:
        s = [token.lemma_ for token in nlp(i)]
        output.append(' '.join(s))
    return output

train['clean_sent'] = lemmatization(train['clean_sent'])

train

import tensorflow_hub as hub
import tensorflow as tf

elmo = hub.Module("https://tfhub.dev/google/elmo/2", trainable=True)

# just a random sentence
x = ["Roasted ants are a popular snack in Columbia"]

# Extract ELMo features 
embeddings = elmo(x, signature="default", as_dict=True)["elmo"]

embeddings.shape

def elmo_vectors(x):
  embeddings = elmo(x.tolist(), signature="default", as_dict=True)["elmo"]

  with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(tf.tables_initializer())
    # return average of ELMo features
    return sess.run(tf.reduce_mean(embeddings,1))

list_train = [train[i:i+100] for i in range(0,train.shape[0],100)]

# Extract ELMo embeddings
elmo_train = [elmo_vectors(x['clean_sent']) for x in list_train]

elmo_train_new = np.concatenate(elmo_train, axis = 0)

import pickle
# save elmo_train_new
pickle_out = open("elmo_cause.pickle","wb")
pickle.dump(elmo_train_new, pickle_out)
pickle_out.close()

# load elmo_train_new
pickle_in = open("elmo_cause.pickle", "rb")
elmo_train_new = pickle.load(pickle_in)

from sklearn.model_selection import train_test_split

xtrain, xvalid, ytrain, yvalid = train_test_split(elmo_train_new, 
                                                  train['label'],  
                                                  random_state=42, 
                                                  test_size=0.1)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score

lreg = LogisticRegression()
lreg.fit(xtrain, ytrain)

# save the model to disk
filename = 'finalized_model.sav'
pickle.dump(lreg, open(filename, 'wb'))

preds_valid = lreg.predict(xvalid)

px = lreg.decision_function(xvalid)

for i in range(0, len(px)):
  print(px[i]," : ",preds_valid[i])

print(metrics.classification_report(yvalid, preds_valid))

import sklearn.metrics as metrics
# calculate the fpr and tpr for all thresholds of the classification
fpr, tpr, threshold = metrics.roc_curve(yvalid, preds_valid)
roc_auc = metrics.auc(fpr, tpr)

# method I: plt
import matplotlib.pyplot as plt
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

f1_score(yvalid, preds_valid)

# make predictions on test set
preds_test = lreg.predict(elmo_test_new)

from sklearn import metrics

metrics.accuiracy_score(test['label'],preds_test)

import nltk
nltk.download("punkt")

text = input("Enter text ")
if not text:
    print("No text entered")
else:
    from nltk import tokenize

    sentences = tokenize.sent_tokenize(text)
    label = []
    for i in range(len(sentences)):
      label.append(0)

    query = pd.DataFrame(list(zip(sentences, label)), 
                columns =['sentences', 'label'])
query['clean_sent'] = query['sentences'].apply(lambda x: re.sub(r'http\S+', '', x))
# remove punctuation marks
punctuation = '!"#$%&()*+-/:;<=>?@[\\]^_`{|}~'

query['clean_sent'] = query['clean_sent'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))

# convert text to lowercase
query['clean_sent'] = query['clean_sent'].str.lower()

# remove whitespaces
query['clean_sent'] = query['clean_sent'].apply(lambda x:' '.join(x.split()))

query['clean_sent'] = lemmatization(query['clean_sent'])

query

list_query = [query[i:i+100] for i in range(0,query.shape[0],100)]

elmo_query = [elmo_vectors(x['clean_sent']) for x in list_query]

elmo_query_new = np.concatenate(elmo_query, axis = 0)

# make predictions on test set
preds_query = lreg.predict(elmo_query_new)
preds_conf = lreg.decision_function(elmo_query_new)

# load the model from disk
loaded_model = pickle.load(open(filename, 'rb'))
result = loaded_model.predict(xvalid)
print(result)

